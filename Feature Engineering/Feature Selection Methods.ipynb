{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征选择(排序)对于数据科学家、机器学习从业者来说非常重要。好的特征选择能够提升模型的性能，更能帮助我们理解数据的特点、底层结构，这对进一步改善模型、算法都有着重要作用。特征选择主要有两个功能：减少特征数量、降维，使模型泛化能力更强，减少过拟合增强对特征和特征值之间的理解拿到数据集，一个特征选择方法，往往很难同时完成这两个目的。通常情况下，我们经常不管三七二十一，选择一种自己最熟悉或者最方便的特征选择方法（往往目的是降维，而忽略了对特征和数据理解的目的）。在许多机器学习相关的书里，很难找到关于特征选择的内容，因为特征选择要解决的问题往往被视为机器学习的一种副作用，一般不会单独拿出来讨论。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 去掉取值变化小(信息量小）的特征 Removing features with low variance\n",
    "\n",
    "这应该是最简单的特征选择方法了：假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[[5.1 1.4 0.2]\n",
      " [4.9 1.4 0.2]\n",
      " [4.7 1.3 0.2]\n",
      " [4.6 1.5 0.2]\n",
      " [5.  1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    " \n",
    "#方差选择法，返回值为特征选择后的数据\n",
    "#参数threshold为方差的阈值\n",
    "a = VarianceThreshold(threshold=0.1).fit_transform(iris.data)\n",
    "b = VarianceThreshold(threshold=0.5).fit_transform(iris.data)\n",
    "print(a[:5])\n",
    "print(b[:5])\n",
    "\n",
    "#方差过小即取值接近一样，可以用方差法筛选连续特征， count（） groupby筛选离散特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 单变量特征选择 Univariate feature selection\n",
    "\n",
    "单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分扔掉不好的特征。对于回归和分类问题可以采用卡方检验等方式对特征进行测试。这种方法比较简单，易于运行，易于理解，通常对于理解数据有较好的效果（但对特征优化、提高泛化能力来说不一定有效）；这种方法有许多改进的版本、变种。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1皮尔逊相关系数\n",
    "皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关。Pearson Correlation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Scipy的pearsonr方法能够同时计算相关系数和p-value，\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower noise (0.7270649532852493, 1.3596948125051596e-50)\n",
      "Higher noise (0.12664606407000215, 0.02828899705921323)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "np.random.seed(4)\n",
    "size = 300\n",
    "x = np.random.normal(0, 1, size)\n",
    "print( \"Lower noise\", pearsonr(x, x + np.random.normal(0, 1, size)))\n",
    "print( \"Higher noise\", pearsonr(x, x + np.random.normal(0, 10, size)))\n",
    "#这个例子中，我们比较了变量在加入噪音之前和之后的差异。当噪音比较小的时候，相关性很强，p-value很低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "缺点：Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.003726086649444559\n",
      "[ 0.57015514 -0.67072523  0.55734183 ... -0.57532697 -0.54593222\n",
      "  0.11249746]\n",
      "[0.32507688 0.44987234 0.31062991 ... 0.33100113 0.29804199 0.01265568]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(-1, 1, 100000)\n",
    "print( pearsonr(x, x**2)[0])\n",
    "print(x)\n",
    "print(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 互信息和最大信息系数 Mutual information and maximal information coefficient (MIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是经典的互信息公式了。想把互信息直接用于特征选择其实不是太方便：1、它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较；2、对于连续变量的计算不是很方便（X和Y都是集合，x，y都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。最大信息系数克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。minepy提供了MIC功能。反过头来看y=x^2这个例子，MIC算出来的互信息值为1(最大的取值)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778881464371471\n"
     ]
    }
   ],
   "source": [
    "from minepy import MINE\n",
    "m = MINE()\n",
    "x = np.random.uniform(-1, 1, 10000)\n",
    "m.compute_score(x, x**2+np.random.normal(0,0.1,10000))\n",
    "print( m.mic())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 距离相关系数 (Distance correlation)\n",
    "距离相关系数是为了克服Pearson相关系数的弱点而生的。在x和x^2这个例子中，即便Pearson相关系数是0，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是0，那么我们就可以说这两个变量是独立的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021189031439749022 0.23051992476158456 0.17849137776786192 0.04733561571533031\n",
      "0.014612967111948975 0.06210319362896604 0.17849137776786192 0.3101927932494554\n",
      "0.011868725405189729 0.038310830847283005 0.3094092687489433 0.3101927932494554\n",
      "0.001311566873843021 0.004207046841521612 0.31332458981728684 0.3101927932494554\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#网上的一个实现\n",
    "\"\"\"\n",
    "Created on Fri Jun 15 14:00:29 2012\n",
    "Author: Josef Perktold\n",
    "License: MIT, BSD-3 (for statsmodels)\n",
    "http://en.wikipedia.org/wiki/Distance_correlation\n",
    "Yaroslav and Satrajit on sklearn mailing list\n",
    "Univariate only, distance measure is just absolute distance\n",
    "Note: Same as R package energy DCOR, except DCOR reports sqrt of all returns of dcov_all\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def dist(x, y):\n",
    "    #1d only\n",
    "    return np.abs(x[:, None] - y)\n",
    "    \n",
    "\n",
    "def d_n(x):\n",
    "    d = dist(x, x)\n",
    "    dn = d - d.mean(0) - d.mean(1)[:,None] + d.mean()\n",
    "    return dn\n",
    "\n",
    "\n",
    "def dcov_all(x, y):\n",
    "    dnx = d_n(x)\n",
    "    dny = d_n(y)\n",
    "    \n",
    "    denom = np.product(dnx.shape)\n",
    "    dc = (dnx * dny).sum() / denom\n",
    "    dvx = (dnx**2).sum() / denom\n",
    "    dvy = (dny**2).sum() / denom\n",
    "    dr = dc / (np.sqrt(dvx) * np.sqrt(dvy))\n",
    "    return dc, dr, dvx, dvy\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for case in range(1,5):\n",
    "\n",
    "    np.random.seed(9854673)\n",
    "    x = np.linspace(-1,1, 501)\n",
    "    if case == 1:\n",
    "        y = - x**2 + 0.2 * np.random.rand(len(x))\n",
    "    elif case == 2:\n",
    "        y = np.cos(x*2*np.pi) + 0.1 * np.random.rand(len(x))\n",
    "    elif case == 3:\n",
    "        x = np.sin(x*2*np.pi) + 0.0 * np.random.rand(len(x))  #circle\n",
    "    elif case == 4:\n",
    "        x = np.sin(x*1.5*np.pi) + 0.1 * np.random.rand(len(x))  #bretzel\n",
    "    dc, dr, dvx, dvy = dcov_all(x, y)\n",
    "    print (dc, dr, dvx, dvy)\n",
    "    \n",
    "    ax = fig.add_subplot(2,2, case)\n",
    "    #ax.set_xlim(-1, 1)\n",
    "    ax.plot(x, y, '.')\n",
    "    yl = ax.get_ylim()\n",
    "    ax.text(-0.95, yl[0] + 0.9 * np.diff(yl), 'dr=%4.2f' % dr)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管有MIC和距离相关系数在了，但当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。第一、Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。第二、Pearson相关系数的取值区间是[-1，1]，而MIC和距离相关系数都是[0，1]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 基于学习模型的特征排序 (Model based ranking)\n",
    "\n",
    "这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。其实Pearson相关系数等价于线性回归里的标准化回归系数。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.623, 'LSTAT'), (0.55, 'RM'), (0.473, 'NOX'), (0.339, 'INDUS'), (0.31, 'TAX'), (0.288, 'PTRATIO'), (0.176, 'CRIM'), (0.168, 'RAD'), (0.148, 'B'), (0.121, 'ZN'), (0.104, 'DIS'), (0.049, 'AGE'), (-0.019, 'CHAS')]\n"
     ]
    }
   ],
   "source": [
    "#运用RF得到特征重要性\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, ShuffleSplit\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    " \n",
    "#Load boston housing dataset as an example\n",
    "boston = load_boston()\n",
    "X = boston[\"data\"]\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    " \n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=5)\n",
    "scores = []\n",
    "for i in range(X.shape[1]):\n",
    "     score = cross_val_score(rf, X[:, i:i+1], Y, scoring=\"r2\",\n",
    "                              cv=ShuffleSplit(len(X), 3, .3))\n",
    "     scores.append((round(np.mean(score), 3), names[i]))\n",
    "print (sorted(scores, reverse=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 线性模型和正则化\n",
    "单变量特征选择方法独立的衡量每个特征与响应变量之间的关系，另一种主流的特征选择方法是基于机器学习模型的方法。有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。说句题外话，这种方法好像在一些地方叫做wrapper类型，大概意思是说，特征排序模型和机器学习模型是耦盒在一起的，对应的非wrapper类型的特征选择方法叫做filter类型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面将介绍如何用回归模型的系数来选择特征。越是重要的特征在模型中对应的系数就会越大，而跟输出变量越是无关的特征对应的系数就会越接近于0。在噪音不多的数据上，或者是数据量远远大于特征数的数据上，如果特征之间相对来说是比较独立的，那么即便是运用最简单的线性回归模型也一样能取得非常好的效果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model: 0.984 * X0 + 1.995 * X1 + -0.041 * X2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    " \n",
    "np.random.seed(0)\n",
    "size = 5000\n",
    " \n",
    "#A dataset with 3 features\n",
    "X = np.random.normal(0, 1, (size, 3))\n",
    "#Y = X0 + 2*X1 + noise\n",
    "Y = X[:,0] + 2*X[:,1] + np.random.normal(0, 2, size)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, Y)\n",
    " \n",
    "#A helper method for pretty-printing linear models\n",
    "def pretty_print_linear(coefs, names = None, sort = False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
    "                                   for coef, name in lst)\n",
    " \n",
    "print (\"Linear model:\", pretty_print_linear(lr.coef_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子当中，尽管数据中存在一些噪音，但这种特征选择模型仍然能够很好的体现出数据的底层结构。当然这也是因为例子中的这个问题非常适合用线性模型来解：特征和响应变量之间全都是线性关系，并且特征之间均是独立的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在很多实际的数据当中，往往存在多个互相关联的特征，这时候模型就会变得不稳定，数据中细微的变化就可能导致模型的巨大变化（模型的变化本质上是系数，或者叫参数，可以理解成W），这会让模型的预测变得困难，这种现象也称为多重共线性。例如，假设我们有个数据集，它的真实模型应该是Y=X1+X2，当我们观察的时候，发现Y’=X1+X2+e，e是噪音。如果X1和X2之间存在线性关系，例如X1约等于X2，这个时候由于噪音e的存在，我们学到的模型可能就不是Y=X1+X2了，有可能是Y=2X1，或者Y=-X1+3X2。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model: -1.291 * X0 + 1.591 * X1 + 2.747 * X2\n"
     ]
    }
   ],
   "source": [
    "#下边这个例子当中，在同一个数据上加入了一些噪音，用随机森林算法进行特征选择。\n",
    "from sklearn.linear_model import LinearRegression\n",
    " \n",
    "size = 100\n",
    "np.random.seed(seed=5)\n",
    " \n",
    "X_seed = np.random.normal(0, 1, size)\n",
    "X1 = X_seed + np.random.normal(0, .1, size)\n",
    "X2 = X_seed + np.random.normal(0, .1, size)\n",
    "X3 = X_seed + np.random.normal(0, .1, size)\n",
    " \n",
    "Y = X1 + X2 + X3 + np.random.normal(0,1, size)\n",
    "X = np.array([X1, X2, X3]).T\n",
    " \n",
    "lr = LinearRegression()\n",
    "lr.fit(X,Y)\n",
    "print (\"Linear model:\", pretty_print_linear(lr.coef_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "系数之和接近3，基本上和上上个例子的结果一致，应该说学到的模型对于预测来说还是不错的。但是，如果从系数的字面意思上去解释特征的重要性的话，X3对于输出变量来说具有很强的正面影响，而X1具有负面影响，而实际上所有特征与输出变量之间的影响是均等的。同样的方法和套路可以用到类似的线性模型上，比如逻辑回归。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso model:  [-0.23616802  0.08100299 -0.          0.54017417 -0.70027816  2.99189989\n",
      " -0.         -1.08067403  0.         -0.         -1.75682067  0.63108483\n",
      " -3.70696598] ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    " \n",
    "boston = load_boston()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(boston[\"data\"])\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    " \n",
    "lasso = Lasso(alpha=.3)\n",
    "lasso.fit(X, Y)\n",
    " \n",
    "print (\"Lasso model: \", lasso.coef_, names)\n",
    "\n",
    "#趋向于0的正则化 L1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso model:  [-0.          0.         -0.          0.         -0.          2.53639988\n",
      " -0.         -0.         -0.         -0.         -1.15959658  0.\n",
      " -3.48704991] ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    " \n",
    "boston = load_boston()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(boston[\"data\"])\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    " \n",
    "lasso = Lasso(alpha=1.3)\n",
    "lasso.fit(X, Y)\n",
    " \n",
    "print (\"Lasso model: \", lasso.coef_, names)\n",
    "\n",
    "#趋向于0的正则化 L1---进一步增大alpha，增大正则力度，更多的0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2：\n",
    "\n",
    "L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 0\n",
      "Linear model: [ 0.7284403   2.30926001 -0.08219169]\n",
      "Ridge model: [0.93832131 1.05887277 0.87652644]\n",
      "Random seed 1\n",
      "Linear model: [ 1.15181561  2.36579916 -0.59900864]\n",
      "Ridge model: [0.98409577 1.06792673 0.75855367]\n",
      "Random seed 2\n",
      "Linear model: [0.69734749 0.32155864 2.08590886]\n",
      "Ridge model: [0.97159124 0.94256202 1.08539406]\n",
      "Random seed 3\n",
      "Linear model: [0.28735446 1.25386129 1.49054726]\n",
      "Ridge model: [0.91891806 1.00474386 1.03276594]\n",
      "Random seed 4\n",
      "Linear model: [0.18726691 0.77214206 2.1894915 ]\n",
      "Ridge model: [0.96401621 0.98152524 1.0983599 ]\n",
      "Random seed 5\n",
      "Linear model: [-1.2912413   1.59097473  2.74727029]\n",
      "Ridge model: [0.75819864 1.01085804 1.1390417 ]\n",
      "Random seed 6\n",
      "Linear model: [ 1.19909595 -0.0306915   1.91454912]\n",
      "Ridge model: [1.01616507 0.89032238 1.0907386 ]\n",
      "Random seed 7\n",
      "Linear model: [ 1.47386769  1.76236014 -0.15057274]\n",
      "Ridge model: [1.0179376  1.03865514 0.90082373]\n",
      "Random seed 8\n",
      "Linear model: [0.0840547  1.87985845 1.10688887]\n",
      "Ridge model: [0.90685834 1.07119752 1.00837994]\n",
      "Random seed 9\n",
      "Linear model: [0.71408648 0.77601368 1.36406398]\n",
      "Ridge model: [0.89617178 0.90340866 0.98015958]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "size = 100\n",
    " \n",
    "#We run the method 10 times with different random seeds\n",
    "for i in range(10):\n",
    "    print (\"Random seed %s\" % i)\n",
    "    np.random.seed(seed=i)\n",
    "    X_seed = np.random.normal(0, 1, size)\n",
    "    X1 = X_seed + np.random.normal(0, .1, size)\n",
    "    X2 = X_seed + np.random.normal(0, .1, size)\n",
    "    X3 = X_seed + np.random.normal(0, .1, size)\n",
    "    Y = X1 + X2 + X3 + np.random.normal(0, 1, size)\n",
    "    X = np.array([X1, X2, X3]).T\n",
    " \n",
    " \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X,Y)\n",
    "    print (\"Linear model:\", (lr.coef_))\n",
    " \n",
    "    ridge = Ridge(alpha=10)\n",
    "    ridge.fit(X,Y)\n",
    "    print (\"Ridge model:\", (ridge.coef_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，不同的数据上线性回归得到的模型（系数）相差甚远，但对于L2正则化模型来说，结果中的系数非常的稳定，差别较小，都比较接近于1，能够反映出数据的内在结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4.随机森林\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 平均不纯度减少 \n",
    "mean decrease impurity随机森林由多个决策树构成。决策树中的每一个节点都是关于某个特征的条件，为的是将数据集按照不同的响应变量一分为二。利用不纯度可以确定节点（最优条件），对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。下边的例子是sklearn中基于随机森林的特征重要度度量方法：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score:\n",
      "[(0.4408, 'LSTAT'), (0.3644, 'RM'), (0.0594, 'DIS'), (0.0463, 'CRIM'), (0.0228, 'NOX'), (0.0172, 'B'), (0.0141, 'TAX'), (0.0136, 'PTRATIO'), (0.0093, 'AGE'), (0.0062, 'INDUS'), (0.003, 'RAD'), (0.0024, 'ZN'), (0.0005, 'CHAS')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "#Load boston housing dataset as an example\n",
    "boston = load_boston()\n",
    "X = boston[\"data\"]\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X, Y)\n",
    "print (\"Features sorted by their score:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names), \n",
    "             reverse=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "缺点：需要注意的一点是，关联特征的打分存在不稳定的现象，这不仅仅是随机森林特有的，大多数基于模型的特征选择方法都存在这个问题。\n",
    "\n",
    "可自行观察上述代码多次运行后输出的顺序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 平均精确率减少 Mean decrease accuracy\n",
    "\n",
    "另一种常用的特征选择方法就是直接度量每个特征对模型精确率的影响。主要思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。很明显，对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的变量来说，打乱顺序就会降低模型的精确率。这个方法sklearn中没有直接提供，但是很容易实现，下面继续在波士顿房价数据集上进行实现。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score:\n",
      "[(0.7285, 'LSTAT'), (0.5754, 'RM'), (0.081, 'DIS'), (0.0412, 'NOX'), (0.0408, 'CRIM'), (0.0209, 'PTRATIO'), (0.0172, 'TAX'), (0.0108, 'AGE'), (0.0059, 'INDUS'), (0.0052, 'B'), (0.0032, 'RAD'), (0.0005, 'CHAS'), (0.0003, 'ZN')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import defaultdict\n",
    " \n",
    "X = boston[\"data\"]\n",
    "Y = boston[\"target\"]\n",
    " \n",
    "rf = RandomForestRegressor()\n",
    "scores = defaultdict(list)\n",
    " \n",
    "#crossvalidate the scores on a number of different random splits of the data\n",
    "for train_idx, test_idx in ShuffleSplit(len(X), 100, .3):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "    r = rf.fit(X_train, Y_train)\n",
    "    acc = r2_score(Y_test, rf.predict(X_test))\n",
    "    for i in range(X.shape[1]):\n",
    "        X_t = X_test.copy()\n",
    "        np.random.shuffle(X_t[:, i])\n",
    "        shuff_acc = r2_score(Y_test, rf.predict(X_t))\n",
    "        scores[names[i]].append((acc-shuff_acc)/acc)\n",
    "print (\"Features sorted by their score:\")\n",
    "print (sorted([(round(np.mean(score), 4), feat) for\n",
    "              feat, score in scores.items()], reverse=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子当中，LSTAT和RM这两个特征对模型的性能有着很大的影响，打乱这两个特征的特征值使得模型的性能下降了73%和57%。注意，尽管这些我们是在所有特征上进行了训练得到了模型，然后才得到了每个特征的重要性测试，这并不意味着我们扔掉某个或者某些重要特征后模型的性能就一定会下降很多，因为即便某个特征删掉之后，其关联特征一样可以发挥作用，让模型性能基本上不变。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 两种顶层特征选择算法\n",
    "之所以叫做顶层，是因为他们都是建立在基于模型的特征选择方法基础之上的，例如回归和SVM，在不同的子集上建立模型，然后汇总最终确定特征得分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 稳定性选择 Stability selection-- 如多次Lasso后汇总权重的结果\n",
    "\n",
    "稳定性选择是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tianjiayang\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class RandomizedLasso is deprecated; The class RandomizedLasso is deprecated in 0.19 and will be removed in 0.21.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score:\n",
      "[(1.0, 'RM'), (1.0, 'PTRATIO'), (1.0, 'LSTAT'), (0.63, 'B'), (0.565, 'CHAS'), (0.37, 'TAX'), (0.365, 'CRIM'), (0.24, 'DIS'), (0.205, 'NOX'), (0.16, 'INDUS'), (0.04, 'ZN'), (0.015, 'RAD'), (0.0, 'AGE')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    " \n",
    "#using the Boston housing data. \n",
    "#Data gets scaled automatically by sklearn's implementation\n",
    "X = boston[\"data\"]\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    " \n",
    "rlasso = RandomizedLasso(alpha=0.025)\n",
    "rlasso.fit(X, Y)\n",
    " \n",
    "print (\"Features sorted by their score:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rlasso.scores_), \n",
    "                 names), reverse=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上边这个例子当中，最高的3个特征得分是1.0，这表示他们总会被选作有用的特征（当然，得分会收到正则化参数alpha的影响，但是sklearn的随机lasso能够自动选择最优的alpha）。接下来的几个特征得分就开始下降，但是下降的不是特别急剧，这跟纯lasso的方法和随机森林的结果不一样。能够看出稳定性选择对于克服过拟合和对数据理解来说都是有帮助的：总的来说，好的特征不会因为有相似的特征、关联特征而得分为0，这跟Lasso是不同的。对于特征选择任务，在许多数据集和环境下，稳定性选择往往是性能最好的方法之一。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 递归特征消除 Recursive feature elimination (RFE)\n",
    "\n",
    "递归特征消除的主要思想是反复的构建模型（如SVM或者回归模型）然后选出最好的（或者最差的）的特征（可以根据系数来选），把选出来的特征放到一遍，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法。RFE的稳定性很大程度上取决于在迭代的时候底层用哪种模型。例如，假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的；假如采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。Sklearn提供了RFE包，可以用于特征消除，还提供了RFECV，可以通过交叉验证来对的特征进行排序。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their rank:\n",
      "[(1, 'NOX'), (2, 'RM'), (3, 'CHAS'), (4, 'PTRATIO'), (5, 'DIS'), (6, 'LSTAT'), (7, 'RAD'), (8, 'CRIM'), (9, 'INDUS'), (10, 'ZN'), (11, 'TAX'), (12, 'B'), (13, 'AGE')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    " \n",
    "boston = load_boston()\n",
    "X = boston[\"data\"]\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    " \n",
    "#use linear regression as the model\n",
    "lr = LinearRegression()\n",
    "#rank all features, i.e continue the elimination until the last one\n",
    "rfe = RFE(lr, n_features_to_select=1)\n",
    "rfe.fit(X,Y)\n",
    " \n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), names)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 一个完整的例子下面将本文所有提到的方法进行实验对比，\n",
    "数据集采用Friedman #1 \n",
    "\n",
    "回归数据（这篇论文中的数据）。\n",
    "\n",
    "数据是用这个公式产生的：\n",
    "\n",
    "X1到X5是由单变量分布生成的，e是标准正态变量N(0,1)。另外，原始的数据集中含有5个噪音变量 X5,…,X10，跟响应变量是独立的。我们增加了4个额外的变量X11,…X14，分别是X1,…,X4的关联变量，通过f(x)=x+N(0,0.01)生成，这将产生大于0.999的关联系数。这样生成的数据能够体现出不同的特征排序方法应对关联特征时的表现。接下来将会在上述数据上运行所有的特征选择方法，并且将每种方法给出的得分进行归一化，让取值都落在0-1之间。对于RFE来说，由于它给出的是顺序而不是得分，我们将最好的5个的得分定为1，其他的特征的得分均匀的分布在0-1之间。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征之间存在线性关联关系，每个特征都是独立评价的，因此X1,…X4的得分和X11,…X14的得分非常接近，而噪音特征X5,…,X10正如预期的那样和响应变量之间几乎没有关系。\n",
    "\n",
    "由于变量X3是二次的，因此X3和响应变量之间看不出有关系（除了MIC之外，其他方法都找不到关系）。\n",
    "\n",
    "这种方法能够衡量出特征和响应变量之间的线性关系，但若想选出优质特征来提升模型的泛化能力，这种方法就不是特别给力了，因为所有的优质特征都不可避免的会被挑出来两次。\n",
    "\n",
    "Lasso能够挑出一些优质特征，同时让其他特征的系数趋于0。当如需要减少特征数的时候它很有用，但是对于数据理解来说不是很好用。\n",
    "\n",
    "（例如在结果表中，X11,X12,X13的得分都是0，好像他们跟输出变量之间没有很强的联系，但实际上不是这样的）\n",
    "\n",
    "MIC对特征一视同仁，这一点上和关联系数有点像，另外，它能够找出X3和响应变量之间的非线性关系。\n",
    "\n",
    "随机森林基于不纯度的排序结果非常鲜明，在得分最高的几个特征之后的特征，得分急剧的下降。\n",
    "\n",
    "从表中可以看到，得分第三的特征比第一的小4倍。而其他的特征选择算法就没有下降的这么剧烈。\n",
    "\n",
    "Ridge将回归系数均匀的分摊到各个关联变量上，从表中可以看出，X11,…,X14和X1,…,X4的得分非常接近。\n",
    "\n",
    "稳定性选择常常是一种既能够有助于理解数据又能够挑出优质特征的这种选择，在结果表中就能很好的看出。像Lasso一样，它能找到那些性能比较好的特征（X1，X2，X4，X5），同时，与这些特征关联度很强的变量也得到了较高的得分。\n",
    "\n",
    "## 总结\n",
    "\n",
    "对于理解数据、数据的结构、特点来说，单变量特征选择是个非常好的选择。尽管可以用它对特征进行排序来优化模型，但由于它不能发现冗余（例如假如一个特征子集，其中的特征之间具有很强的关联，那么从中选择最优的特征时就很难考虑到冗余的问题）。\n",
    "\n",
    "正则化的线性模型对于特征理解和特征选择来说是非常强大的工具。L1正则化能够生成稀疏的模型，对于选择特征子集来说非常有用；相比起L1正则化，L2正则化的表现更加稳定，由于有用的特征往往对应系数非零，因此L2正则化对于数据的理解来说很合适。\n",
    "\n",
    "由于响应变量和特征之间往往是非线性关系，可以采用basis expansion的方式将特征转换到一个更加合适的空间当中，在此基础上再考虑运用简单的线性模型。\n",
    "\n",
    "随机森林是一种非常流行的特征选择方法，它易于使用，一般不需要feature engineering、调参等繁琐的步骤，并且很多工具包都提供了平均不纯度下降方法。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。尽管如此，这种方法仍然非常值得在你的应用中试一试。\n",
    "\n",
    "特征选择在很多机器学习和数据挖掘场景中都是非常有用的。在使用的时候要弄清楚自己的目标是什么，然后找到哪种方法适用于自己的任务。当选择最优特征以提升模型性能的时候，可以采用交叉验证的方法来验证某种方法是否比其他方法要好。\n",
    "\n",
    "当用特征选择的方法来理解数据的时候要留心，特征选择模型的稳定性非常重要，稳定性差的模型很容易就会导致错误的结论。对数据进行二次采样然后在子集上运行特征选择算法能够有所帮助，如果在各个子集上的结果是一致的，那就可以说在这个数据集上得出来的结论是可信的，可以用这种特征选择模型的结果来理解数据。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 2)\n",
      "features sorted by their rank:\n",
      "[(1, 'NOX'), (2, 'RM'), (3, 'CHAS'), (4, 'PTRATIO'), (5, 'DIS'), (6, 'LSTAT'), (7, 'RAD'), (8, 'CRIM'), (9, 'INDUS'), (10, 'ZN'), (11, 'TAX'), (12, 'B'), (13, 'AGE')]\n",
      "(150, 4)\n",
      "(150, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'输出：\\n            (150,4)\\n            (150,3)\\n            '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"1.过滤型\"\"\"\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "iris=load_iris()\n",
    "X,y=iris.data,iris.target\n",
    "print (X.shape)\n",
    "#卡方检验\n",
    "X_new=SelectKBest(chi2,k=2).fit_transform(X,y)\n",
    "print (X_new.shape)\n",
    "\n",
    "\"\"\"输出：\n",
    "        (150L, 4L)\n",
    "        (150L, 2L)\"\"\"\n",
    "\n",
    "\"\"\"2.包裹型\"\"\"\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston=load_boston()\n",
    "X=boston[\"data\"]\n",
    "Y=boston[\"target\"]\n",
    "names=boston[\"feature_names\"]\n",
    "\n",
    "lr=LinearRegression()\n",
    "rfe=RFE(lr,n_features_to_select=1)#选择剔除1个\n",
    "rfe.fit(X,Y)\n",
    "\n",
    "print (\"features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x:round(x,4), rfe.ranking_),names)))\n",
    "\n",
    "\"\"\"输出：按剔除后AUC排名给出\n",
    "features sorted by their rank:\n",
    "[(1.0, 'NOX'), (2.0, 'RM'), (3.0, 'CHAS'), (4.0, 'PTRATIO'), (5.0, 'DIS'), (6.0, 'LSTAT'), (7.0, 'RAD'), (8.0, 'CRIM'), (9.0, 'INDUS'), (10.0, 'ZN'), (11.0, 'TAX')\n",
    ", (12.0, 'B'), (13.0, 'AGE')]\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"3.嵌入型 ，老的版本没有SelectFromModel\"\"\"\n",
    "from sklearn.svm import  LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "iris=load_iris()\n",
    "X,y=iris.data,iris.target\n",
    "print (X.shape)\n",
    "\n",
    "lsvc=LinearSVC(C=0.01,penalty='l1',dual=False).fit(X,y)\n",
    "model=SelectFromModel(lsvc,prefit=True)\n",
    "X_new=model.transform(X)\n",
    "print (X_new.shape)\n",
    "\n",
    "\"\"\"输出：\n",
    "            (150,4)\n",
    "            (150,3)\n",
    "            \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
